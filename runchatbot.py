# -*- coding: utf-8 -*-
"""runChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1APlJpe9IJ43Ym3ZKH2mg2Jz-VauC3Z4C
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')

# %cd /gdrive/My\ Drive/Chatbot

# %ls

!pip install slack_bolt
!pip install langchain
!pip install OpenAI
!pip install faiss-cpu
!pip install tiktoken
!pip install InstructorEmbedding
!pip install sentence_transformers

from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts.prompt import PromptTemplate
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
import os
import re
import pickle

os.environ['OPENAI_API_KEY'] = 'XXXXXX'
os.environ['SLACK_BOT_TOKEN'] = 'XXXXXX'
os.environ['SLACK_APP_TOKEN'] = 'XXXXXX'

# Load the data from the vector storage
with open("faiss_store_amazon_fullproducts.pkl", "rb") as f:
    vectorStore = pickle.load(f)

"""# Run the SlackBot"""

condense_question_prompt = PromptTemplate.from_template(
    """
    Just forward the following question, don't try to summarize a standalone question or make up an answer.

    {question}
    """
)

system_message_prompt = SystemMessagePromptTemplate.from_template(
"""You are a friendly, conversational shopping product assistant from Amazon.
    1. Your duty is to use the context below to show customers what we offer
        that matches their requests, help them find what they want,
        and answer any question that they have below.

    2. When making up an answer for the customer, don't include chat history,
        the customer's original question and yourself (as an assistant).

    3. If you don't know the answer, just say that you don't know,
        don't try to make up an answer.
    Chat History:
    {chat_history}

    Context:
    {context}

    Question:
    {question}
"""
)

# Set up the memory buffer to store the conversation
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Set up the a document-retrieval conversation Chain for the chat bot
chatbot = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo"),
    verbose = True,
    retriever=vectorStore.as_retriever(),
    memory=memory,
    condense_question_prompt=condense_question_prompt,
    combine_docs_chain_kwargs= {
        "prompt": ChatPromptTemplate.from_messages([
            system_message_prompt
        ])
    }
)

# This is the code for running the slack bot, using Flask framework of Python

# Initializes app with the slack bot token and socket mode handler
app = App(token=os.environ.get("SLACK_BOT_TOKEN"))


# Handle events when someone calls the chatbot
@app.event("app_mention")
def handle_app_mention_events(body, say, logger):
    # forward the message to the message_handler
    message_handler(body['event'], say, logger)


# Direct message handler for slack bot
@app.message(".*")
def message_handler(message, say, logger):
    # remove the "@..." tag
    msg = re.sub("<[^>]*>", "", message['text'])
    # generate the results
    result = chatbot.run(question=msg)
    print(result)
    say(result)


# Start the app
if __name__ == "__main__":
    SocketModeHandler(app, os.environ["SLACK_APP_TOKEN"]).start()

"""# Testing New implementations

## Old (Dropped)
"""

# create a chat history buffer
chat_history = []

# gather user input for the first question to kick off the bot
question = input("Hi! What are you looking for today?")

# keep the bot running in a loop to simulate a conversation
while True:
    result = chatbot(
        {"question": question, "chat_history": chat_history}
    )
    print("\n")
    chat_history.append((result["question"], result["answer"]))
    question = input()

"""## Currently Testing"""

condense_question_prompt = PromptTemplate.from_template(
    """
    Just forward the following chat history and question, don't try to summarize a standalone question.

    Chat history:
    {chat_history}

    question:
    {question}

    """
)

system_message_prompt = SystemMessagePromptTemplate.from_template(
    """You are a friendly, conversational retail shopping assistant from Amazon.
Use the following context including product names, product types, descriptions,
and keywords to show the shopper what is available, help them find what they want, and answer any questions they have.

If you don't know the answer, just say that you don't know, don't try to make up an answer.

Chat History:

{chat_history}

Context:

{context}

Human Question:

{question}

"""
)


memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    verbose = True,
    retriever=vectorStore.as_retriever(),
    memory=memory,

    condense_question_prompt=condense_question_prompt,
    combine_docs_chain_kwargs= {
        "prompt": ChatPromptTemplate.from_messages([
            system_message_prompt
        ])
    }
)

# question = "Hi I want to buy a pair of shoes"
# question = "I am looking for a pair of men shoes with US size 8"
# question = "Do you have sneakers?"
# question = "Great, I found what I want, thank you."

# question = "Hi I want to buy a phone case."
question = "I have an iPhone 12 pro."

result = chain.run(question)
result
